{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing is very important in Machine learning as an accurate result also depends on the quality of data \n",
    "\n",
    "This notebook works on preprocessing the dataset and saving it in the npz format for the machine learning algorithm.\n",
    "\n",
    "It makes sense to shuffle the indices prior to balancing the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "## NB: sklearn preprocessing library is used to standardize the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Load the data\n",
    "raw_data = np.loadtxt('Audiobooks_dataset.csv',delimiter=',')\n",
    "#raw_data\n",
    "# alternatively the dataset can be loaded in csv format using pandas as:\n",
    "## raw_data = pd.read_csv(\"Audiobooks_data.csv\", delimiter=',')\n",
    "\n",
    "# The inputs are all columns in the csv, except for the first one [:,0]\n",
    "# (which is just the arbitrary customer IDs that bear no useful information),\n",
    "# and the last one [:,-1] (which is the targets)\n",
    "unscaled_inputs = raw_data[:,1:-1]\n",
    "\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "targets_all = raw_data[:,-1]\n",
    "targets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Shuffle the dataset\n",
    "\n",
    "The indices of the dataset needs to be shuffled the before balancing.\n",
    "\n",
    "The dataset will again be shuffled after balancing otherwise, all targets that are 1s will be contained in the train_targets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1620.   1620.      5.33 ...    0.      0.    249.  ]\n",
      " [ 324.    324.      5.33 ...    0.      0.     12.  ]\n",
      " [2160.   2160.      5.33 ...    0.     30.     41.  ]\n",
      " ...\n",
      " [2160.   2160.      5.33 ...  113.4     0.    157.  ]\n",
      " [ 324.    324.      7.47 ...    0.      0.      0.  ]\n",
      " [2160.   2160.      5.33 ...    0.      0.     82.  ]]\n"
     ]
    }
   ],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# It is therefore necessary to shuffle the indices of the data, \n",
    "# so that the data is not arranged in any way when it is being fed to the algorithm.\n",
    "# Also, because there will be batching, it is required for the data to be as randomly spread out as possible\n",
    "\n",
    "# Grab the indices of the dataset in order  \n",
    "shuffled_indices = np.arange(unscaled_inputs.shape[0]) \n",
    "\n",
    "# randomly shuffle the indices extracted\n",
    "np.random.shuffle(shuffled_indices)\n",
    "#print(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "unscaled_inputs = unscaled_inputs[shuffled_indices]\n",
    "targets_all = targets_all[shuffled_indices]\n",
    "print(unscaled_inputs)\n",
    "#print(targets_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1620.   1620.      5.33 ...    0.      0.    249.  ]\n",
      " [ 324.    324.      5.33 ...    0.      0.     12.  ]\n",
      " [2160.   2160.      5.33 ...    0.     30.     41.  ]\n",
      " ...\n",
      " [1620.   1620.      8.   ...    0.      0.      9.  ]\n",
      " [1188.   1188.      5.87 ...    0.      0.    208.  ]\n",
      " [2160.   2160.     10.13 ...    0.      0.     71.  ]]\n"
     ]
    }
   ],
   "source": [
    "# In balancing the dataset, we want to ensure that there are almost equal number of \"1s\" as there are \"0s\"\n",
    "\n",
    "# First count how many targets are 1 (meaning that the customer did convert)\n",
    "ones_in_target = int(np.sum(targets_all))\n",
    "\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "target_zeros_counter = 0\n",
    "\n",
    "# Surplus 0s or 1s need to be removed from the dataset \n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "\n",
    "for i in range(targets_all.shape[0]): \n",
    "    if targets_all[i] == 0:\n",
    "        target_zeros_counter += 1\n",
    "        if target_zeros_counter > ones_in_target:\n",
    "            indices_to_remove.append(i)  # store the indices to be removed\n",
    "\n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \"to remove\" in the loop above.\n",
    "balanced_unscaled_inputs = np.delete(unscaled_inputs, indices_to_remove, axis=0)\n",
    "balanced_targets = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "print(balanced_unscaled_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22939327e-01 -2.44379273e-01 -3.41337700e-01 ... -4.45110996e-01\n",
      "  -1.40423545e-01  1.91454550e+00]\n",
      " [-2.47657078e+00 -1.72705917e+00 -3.41337700e-01 ... -4.45110996e-01\n",
      "  -1.40423545e-01 -6.34836635e-01]\n",
      " [ 1.20606854e+00  3.73404018e-01 -3.41337700e-01 ... -4.45110996e-01\n",
      "   4.56062953e+01 -3.22886922e-01]\n",
      " ...\n",
      " [ 1.22939327e-01 -2.44379273e-01  1.43309424e-01 ... -4.45110996e-01\n",
      "  -1.40423545e-01 -6.67107295e-01]\n",
      " [-7.43564041e-01 -7.38605906e-01 -2.43319180e-01 ... -4.45110996e-01\n",
      "  -1.40423545e-01  1.47351315e+00]\n",
      " [ 1.20606854e+00  3.73404018e-01  5.29938028e-01 ... -4.45110996e-01\n",
      "  -1.40423545e-01 -1.80323312e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Th input data is now standardized using the sklearn preprocessing library\n",
    "scaled_inputs = preprocessing.scale(balanced_unscaled_inputs)\n",
    "print(scaled_inputs)\n",
    "# N/B: I only standardized the inputs and not the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = balanced_targets[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685.0 3355 0.5022354694485842\n",
      "325.0 671 0.4843517138599106\n",
      "227.0 447 0.5078299776286354\n"
     ]
    }
   ],
   "source": [
    "# In this step the data would be split into 75% training, 15% validation and 10% testing\n",
    "\n",
    "# Get the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Get the number of samples for the train_data (75%)\n",
    "train_samples_count = int(0.75 * samples_count)\n",
    "validation_samples_count = int(0.15 * samples_count)\n",
    "\n",
    "# The remaining dataset would be for the testing dataset.\n",
    "test_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count] #. the first 75% of the dataset\n",
    "train_targets = shuffled_targets[:train_samples_count] # the first 75% of the dataset\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# # Accounts for the next 15% of the dataset\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# The dataset was balanced to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  STEP 7: Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "# The data format was saved coherently for easy access\n",
    "\n",
    "# The saved dataset can be found in this directory\n",
    "\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
